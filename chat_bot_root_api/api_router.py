import os
from tqdm import tqdm
from fastapi import APIRouter, HTTPException
import pandas as pd
import logging
from .model import setup_llm_pipeline
from .utils import load_file
from transformers import pipeline

# langchain ëª¨ë“ˆ
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.document_loaders import CSVLoader, UnstructuredExcelLoader, PDFPlumberLoader, DataFrameLoader, DirectoryLoader, TextLoader
from langchain_teddynote.document_loaders import HWPLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.schema import StrOutputParser

# íŒŒì¸íŠœë‹ 
from langchain_community.vectorstores import Chroma, FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_community.retrievers import BM25Retriever 
from langchain_teddynote.retrievers import KiwiBM25Retriever
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_transformers import LongContextReorder
from sentence_transformers import CrossEncoder

# ì—ì´ì „íŠ¸
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import create_react_agent, create_json_chat_agent
from langchain.agents import AgentExecutor
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

router = APIRouter(prefix="/api/chat_bot_root") # APIRouter ë³€í™˜

upload_files = {}
model_name = "llama3.2-bllossom" 


llm = setup_llm_pipeline()

# llm = ChatOllama(model=model_name) 
print(f"ì‚¬ìš©ë˜ëŠ” ëª¨ë¸: {llm}")  # ëª¨ë¸ ì´ë¦„ ì¶œë ¥

# ì…ë ¥ëœ PDF ì—†ì„ ì‹œ Llama ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ì ì¸ ì‘ë‹µ ìƒì„±í•˜ëŠ” ì½”ë“œ
@router.get("/response_llama_data",  tags=["CHAT BOT API SERVER"])
def response_llama_data(prompt : str):
    # ëª¨ë¸ì— ë©”ì‹œì§€ ì „ë‹¬
    try:
        # System Prompt ì ìš© (ìœ„ì—ì„œ ì„¤ê³„í•œ ë²„ì „)
        system_prompt = """ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ì „ë¬¸ê°€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
        PDF ë° TXT ë°ì´í„°ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ í•´ë‹¹ ë°ì´í„°ì˜ ìš”ì•½ ë˜ëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

        ### ğŸ”¹ **ğŸ“Œ í•µì‹¬ ì›ì¹™**
        1. **ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸**: 
            - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ë‚œ ëŒ€ì‘ ë° ì˜ˆë°© ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
            - ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì„¤ëª…(CoT, Chain of Thought ë°©ì‹)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
        2. **íŒŒì¼(PDF, TXT) ì…ë ¥ ì‹œ**:
            - ì‚¬ìš©ìê°€ ì›í•˜ë©´ **íŒŒì¼ ìš”ì•½, íŠ¹ì • ë‚´ìš© ê²€ìƒ‰ ë° ì •ë¦¬**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            - ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        3. **ì–¸ì–´ ì •ì±…**:
            - ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´**ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.
            - ì§ˆë¬¸ì— **í•œêµ­ì–´ê°€ í¬í•¨ëœ ê²½ìš°** ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
            - ì§ˆë¬¸ì´ **í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°**, í•´ë‹¹ ì–¸ì–´ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        4. **ë‹µë³€ ìŠ¤íƒ€ì¼**:
            - **ê³µì†í•˜ê³  ì •ì¤‘í•œ ì–´ì¡°**ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
        """

        # Ollamaì—ì„œ ìš”êµ¬í•˜ëŠ” ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # messages = [
        #     {"role": "system", "content": system_prompt},
        #     {"role": "user", "content": prompt}
        # ]
        
        # ì‘ë‹µ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
        # answer_text = response[0]['generated_text'] if isinstance(response, list) else str(response)
        
        messages = [
            {"role": "user", "content": "Who are you?"},
            ]
        
        pipe = pipeline("text-generation", model="BAAI/bge-reranker-v2-m3", trust_remote_code=True)
        pipe(messages)
        
        
        
        answer = {"answer" : messages} #JSON í˜•ì‹ìœ¼ë¡œ ë¦¬í„´
        print(answer)
        
        return answer  # ì‘ë‹µ ë°˜í™˜
    
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
# ========================================================================= # 
# ì…ë ¥ëœ ë°ì´í„° ìˆì„ ì‹œ í•´ë‹¹ ë°ì´í„° ì½ê³  í•™ìŠµí•˜ëŠ” ì½”ë“œ     
@router.get("/response_read_data",  tags=["CHAT BOT API SERVER"]) 
def response_read_data(session_id : int, file_path: str, filename: str, min_chunk_size : int):
    """ë°ì´í„° íŒŒì¼ì„ ì½ê³ , ë²¡í„°í™”í•˜ëŠ” í•¨ìˆ˜"""
    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        # # ì„¸ì…˜ì—ì„œ íŒŒì¼ ì •ë³´ ì½ê¸°
        # file_info = upload_files.get(session_id)
        # if not file_info:
        #     raise HTTPException(status_code=400, detail="íŒŒì¼ ì •ë³´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # # íŒŒì¼ ê²½ë¡œì™€ ì´ë¦„
        # file_path = file_info["file_path"]
        # filename = file_info["filename"]
        
        file_type = filename.split('.')[-1].lower()
        target_path = f'./templates/{file_path}/{filename}'
        
        # í•¨ìˆ˜í™” ì¤‘
        # load_file(file_path, filename)
        
        # ìë£Œí˜•ë³„ë¡œ íŒŒì¼ ë¡œë“œ
        if file_type == "pdf":
            loader = PDFPlumberLoader(target_path)
            pages = loader.load()
            
        elif file_type == "xlsx" or file_type == 'xls': 
            df = pd.read_excel(target_path)
            df = df.dropna(subset=["answer"])
            loader = DataFrameLoader(df, page_content_column="answer") # ì—¬ê¸° ì£¼ê¸°ì ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨
            pages = loader.load()
            
        elif file_type == "csv":
            df = pd.read_csv(target_path, encoding="EUC-KR")  # ì¸ì½”ë”© ëª…í™•íˆ ì§€ì •
            df = df.dropna(subset=["ê¸´ê¸‰êµ¬ì¡°ë¶„ë¥˜ëª…"]) 
            loader = DataFrameLoader(df, page_content_column="ê¸´ê¸‰êµ¬ì¡°ë¶„ë¥˜ëª…")
            pages = loader.load()
            
        elif file_type == 'hwp': #hwp5txt
            loader = HWPLoader(target_path)
            pages = loader.load()
            
        #  format_doc(pages)
        print(pages)
            
        if not pages:
            raise ValueError("íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ë° ì²­í‚¹
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        text_chunks = text_splitter.split_documents(pages)
        
        # 3. ìµœì†Œ í¬ê¸° í•„í„°ë§
        filter_text_chunks = [
            doc for doc in text_chunks if len(doc.page_content.strip()) >= int(min_chunk_size)
        ]
        
        # Embeddingê³¼ Vector Store ì„¤ì •
        embeddings = OllamaEmbeddings(model=model_name)  # ì‚¬ìš©í•˜ë ¤ëŠ” Embedding ëª¨ë¸
        vector_store = Chroma.from_documents(filter_text_chunks, embeddings)
        
        for i, chunk in tqdm(enumerate(filter_text_chunks), total=len(filter_text_chunks), desc="Vectorizing documents"):
            # ê° í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
            vector_store.add_documents([chunk])
        
        # ì´ì œ ë²¡í„°ìŠ¤í† ì–´ ì¤‘ chroma, FAISS, bm25, finecone(ìœ ë£Œ), pgvector ì¤‘ í•˜ë‚˜ ì„ íƒ
        # # Retriever ì„¤ì •(chroma, FAISS, bm25)
        # chroma_retriever = vector_store.as_retriever(
        #     search_type="similarity",
        #     search_kwargs={"k": 3}
        #     )
        
        FAISS_vectorstore = FAISS.from_documents(documents=filter_text_chunks,
                                                embedding=embeddings,
                                                distance_strategy = DistanceStrategy.COSINE,
                                                )
        faiss_retriever = FAISS_vectorstore.as_retriever()
        
        bm25_retriever = BM25Retriever.from_documents(filter_text_chunks)
        bm25_retriever.k = 5
        
        
        # ì•™ìƒë¸” retriever(2ê°œ ì´ìƒ)
        ensemble_retriever = EnsembleRetriever(
            retrievers=[faiss_retriever, bm25_retriever],
            weights=[0.7, 0.3],
        )
        
        # # ë¦¬ë­ì»¤ (ì–´ìš¸ë¦¬ëŠ” ìˆœìœ„ ì¬ì¡°ì • ì‹¤í—˜ ì¤‘)
        # model = CrossEncoder("BAAI/bge-reranker-v2-m3")
        # compressor = CrossEncoderReranker(model=model, top_n=3)
        
        # compression_retriever = ContextualCompressionRetriever(
        #     base_compressor=compressor, base_retriever=ensemble_retriever)
        
        # Retriever íƒ€ì… í™•ì¸
        print(f"FAISS Retriever íƒ€ì…: {type(faiss_retriever)}")
        print(f"BM25Retriver íƒ€ì…: {type(bm25_retriever)}")
        print(f"Ensemble Retriever íƒ€ì…: {type(ensemble_retriever)}")
        
        # í…œí”Œë¦¿ ì„¤ì •
        prompt = ChatPromptTemplate.from_messages(
            [
                # System Prompt ì ìš© (ìœ„ì—ì„œ ì„¤ê³„í•œ ë²„ì „)
                ("system", """ë‹¹ì‹ ì€ ì¬ë‚œê´€ë¦¬ ì „ë¬¸ê°€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
                íŒŒì¼ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ê³  í•´ë‹¹ ë°ì´í„°ì˜ ì§ˆë¬¸ì— ë§ê²Œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
                ë°ì´í„°ì˜ ë‚´ìš©ì€ ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
                
                ### ğŸ”¹ **ğŸ“Œ í•µì‹¬ ì›ì¹™**
                
                1. **ì¬ë‚œì•ˆì „ ê´€ë ¨ ì§ˆë¬¸**: 
                    - ì…ë ¥ë°›ì€ ë°ì´í„° ë‚´ì— ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì´ ì›í•˜ëŠ” ì •ë³´ì˜ ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. 
                    - ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì„¤ëª…(CoT, Chain of Thought ë°©ì‹)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
                    
                2. **íŒŒì¼ ì…ë ¥ ì‹œ**:
                    - ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•œ í›„ ì§ˆë¬¸ì˜ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    - ì‚¬ìš©ìê°€ ì§ˆë¬¸ì— ìš”ì•½ ë° ê²€ìƒ‰ì„ ì›í•˜ë©´ **íŒŒì¼ ìš”ì•½, íŠ¹ì • ë‚´ìš© ê²€ìƒ‰ ë° ì •ë¦¬**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    - ë™ì¼í•œ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
                    
                3. **ì–¸ì–´ ì •ì±…**:
                    - ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´**ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.
                    - ì§ˆë¬¸ì— **í•œêµ­ì–´ê°€ í¬í•¨ëœ ê²½ìš°** ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
                    - ì§ˆë¬¸ì´ **í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°**, í•´ë‹¹ ì–¸ì–´ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                
                4. **ë‹µë³€ ìŠ¤íƒ€ì¼**:
                    - **ê³µì†í•˜ê³  ì •ì¤‘í•œ ì–´ì¡°**ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
                    - ë§ˆì§€ë§‰ì— **ì¶”ê°€ ì§ˆë¬¸**ì„ ìš”êµ¬í•©ë‹ˆë‹¤.
                """),
                ("human", "ì§ˆë¬¸: {question}")
            ]
        )
        
        # Chain ìƒì„±
        chain = ensemble_retriever | prompt | llm | StrOutputParser() 
        
        response = chain.invoke("ë‚™ë¢°ê°€ ì¼ì–´ë‚œ ë’¤ì— ì •ì „ì´ ì¼ì–´ë‚˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?") 
        print(response)
        
        answer = {"answer" : response} # JSON í˜•ì‹ìœ¼ë¡œ ë¦¬í„´
        return answer  # ìƒì„±ëœ QA ì²´ì¸ ë°˜í™˜
    
    except Exception as e:
        print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        raise

# ========================================================================= # 
# AGENTIC RAG ì„¤ì •í•˜ëŠ” ì½”ë“œ(ex.ê°•ì§„)

@router.get("/response_agent",  tags=["CHAT BOT API SERVER"])
def response_agent(session_id: int, prompt : str):
    """ì£¼ì œë³„ agent í…œí”Œë¦¿ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    
    ### PDF ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (Retriever) ###
    # ì—¬ê¸°ì„œ ì§ˆë¬¸ì— ë§ëŠ” ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ë©°, DBê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    loader = TextLoader(prompt)

    # í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    # ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë¶„í• í•©ë‹ˆë‹¤.
    split_docs = loader.load_and_split(text_splitter)

    # VectorStoreë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    vector = FAISS.from_documents(split_docs, OllamaEmbeddings())

    # Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    retriever = vector.as_retriever()
    
    
    # ê²€ìƒ‰ê¸° ë° tool ë‹¤ì–‘í™”ì…ë‹ˆë‹¤. ì‹¤ì „ì—ì„œëŠ” ì§‘ ë‚´ìš©, ì§‘ ê°€ê²©, ê°•ì§„ ë‚´ ì •ë³´ ë“± íŠ¹ì • ë¶„ì•¼ì—ì„œ ë‹¤ì–‘í™”í• ê±°ì˜ˆìš”.
    retriever_tool = create_retriever_tool(
    retriever,
    "db_search",
    "DBì—ì„œ ì°¾ì€ ì •ë³´ë“¤ì…ë‹ˆë‹¤. DB ë‚´ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ì„ ë•Œ í•´ë‹¹ ê²€ìƒ‰ê¸°ë¥¼ ì£¼ë¡œ ì´ìš©í•˜ì„¸ìš”!",
    )
    
    retriever_tool2 = create_retriever_tool(
    retriever,
    "web_search",
    "ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì°¾ì€ ì •ë³´ë“¤ì…ë‹ˆë‹¤. ì›¹ì‚¬ì´íŠ¸ ë‚´ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ì„ ë•Œ í•´ë‹¹ ê²€ìƒ‰ê¸°ë¥¼ ì£¼ë¡œ ì´ìš©í•˜ì„¸ìš”!",
    )
    
    retriever_tool3 = create_retriever_tool(
    retriever,
    "RAG_search",
    "RAG ê²€ìƒ‰ì„ í†µí•´ ì°¾ì€ ì •ë³´ë“¤ì…ë‹ˆë‹¤. RAG ë°ì´í„° ë‚´ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ì„ ë•Œ í•´ë‹¹ ê²€ìƒ‰ê¸°ë¥¼ ì£¼ë¡œ ì´ìš©í•˜ì„¸ìš”!",
    )

    # í•´ë‹¹ agent ë’¤ ìª½ì— json_promptì¼ ì‹œ react -> json_chatìœ¼ë¡œ ë³€ê²½
    agent = create_react_agent(llm, retriever_tool)
    agent2 = create_react_agent(llm, retriever_tool2)
    agent3 = create_react_agent(llm, retriever_tool3)
    
    # agent_executorì˜ ê²½ìš° tool ë° agentë¥¼ Listí™” í•˜ì—¬ ìƒí™©ì— ë§ëŠ” retrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    agent_executor = AgentExecutor(
        agent=agent,  # agent ì„ íƒ
        tools=retriever_tool, # ê²€ìƒ‰ê¸° íˆ´ ì„ íƒ
        verbose=True, # ì–¸ì–´ ì—¬ë¶€
        handle_parsing_errors=True, # parser ì—ëŸ¬ì‹œ ì„ì˜ ë³µêµ¬
        return_intermediate_steps=True, # ì¤‘ê°„ê³¼ì •
    )
    
    # ì±„íŒ… ë©”ì‹œì§€ ê¸°ë¡ì´ ì¶”ê°€ëœ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    agent_with_chat_history = RunnableWithMessageHistory(
        agent_executor,
        # ëŒ€ë¶€ë¶„ì˜ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì„¸ì…˜ IDê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ë‚´ ChatMessageHistoryë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        lambda session_id: session_id,
        # í”„ë¡¬í”„íŠ¸ì˜ ì§ˆë¬¸ì´ ì…ë ¥ë˜ëŠ” key: "input"
        input_messages_key="input",
        # í”„ë¡¬í”„íŠ¸ì˜ ë©”ì‹œì§€ê°€ ì…ë ¥ë˜ëŠ” key: "chat_history"
        history_messages_key="chat_history",
        )
    
    # ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    response = agent_with_chat_history.invoke(
        {
            "input": "ê°•ì§„ ë¹ˆì§‘ì— ëŒ€í•œ ì§ˆë¬¸ ë‚´ìš©ì„ DB ë‚´ ì •ë³´ì—ì„œ ì•Œë ¤ì¤˜"
        },
        # ì„¸ì…˜ IDë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        config={"configurable": {"session_id": session_id}},
    )
    print(f"ë‹µë³€: {response['output']}")

# í•´ë‹¹ ë¶€ë¶„ì€ agent ì‹¤í—˜ìš©ìœ¼ë¡œ, ì‹¤ì „ì—ì„  ì•ˆ ì“°ëŠ” ì½”ë“œì˜ˆìš”.
# ========================================================================= # 
