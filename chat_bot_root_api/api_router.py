import os
from tqdm import tqdm
from fastapi import APIRouter
import pandas as pd
from .model import setup_llm_pipeline
from .utils import preprocess_text

# langchain ëª¨ë“ˆ
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.document_loaders import CSVLoader, TextLoader, PDFPlumberLoader, DataFrameLoader, DirectoryLoader
from langchain_teddynote.document_loaders import HWPLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.schema import StrOutputParser

# íŒŒì¸íŠœë‹ 
from langchain_community.vectorstores import Chroma, FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_community.retrievers import BM25Retriever 
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.document_transformers import LongContextReorder
from sentence_transformers import CrossEncoder


router = APIRouter(prefix="/api/chat_bot_root")

model_name = "llama3.2-bllossom" 

# ì…ë ¥ëœ PDF ì—†ì„ ì‹œ Llama ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ì ì¸ ì‘ë‹µ ìƒì„±í•˜ëŠ” ì½”ë“œ
@router.get("/response_llama_data",  tags=["CHAT BOT API SERVER"])
def response_llama_data(prompt : str):
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = ChatOllama(model=model_name)

    # System Prompt ì ìš© (ìœ„ì—ì„œ ì„¤ê³„í•œ ë²„ì „)
    system_prompt = """ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ì „ë¬¸ê°€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
    PDF ë° TXT ë°ì´í„°ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ í•´ë‹¹ ë°ì´í„°ì˜ ìš”ì•½ ë˜ëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ### ğŸ”¹ **ğŸ“Œ í•µì‹¬ ì›ì¹™**
    1. **ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸**: 
       - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ë‚œ ëŒ€ì‘ ë° ì˜ˆë°© ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
       - ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì„¤ëª…(CoT, Chain of Thought ë°©ì‹)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    2. **íŒŒì¼(PDF, TXT) ì…ë ¥ ì‹œ**:
       - ì‚¬ìš©ìê°€ ì›í•˜ë©´ **íŒŒì¼ ìš”ì•½, íŠ¹ì • ë‚´ìš© ê²€ìƒ‰ ë° ì •ë¦¬**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
       - ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    3. **ì–¸ì–´ ì •ì±…**:
       - ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´**ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.
       - ì§ˆë¬¸ì— **í•œêµ­ì–´ê°€ í¬í•¨ëœ ê²½ìš°** ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
       - ì§ˆë¬¸ì´ **í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°**, í•´ë‹¹ ì–¸ì–´ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    4. **ë‹µë³€ ìŠ¤íƒ€ì¼**:
       - **ê³µì†í•˜ê³  ì •ì¤‘í•œ ì–´ì¡°**ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    """

    # Ollamaì—ì„œ ìš”êµ¬í•˜ëŠ” ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    # ëª¨ë¸ì— ë©”ì‹œì§€ ì „ë‹¬
    try:
        response = model.invoke(messages)
        return response  # ì‘ë‹µ ë°˜í™˜
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
# ========================================================================= # 
# ì…ë ¥ëœ ë°ì´í„° ìˆì„ ì‹œ í•´ë‹¹ ë°ì´í„° ì½ê³  í•™ìŠµí•˜ëŠ” ì½”ë“œ     
@router.get("/response_read_data",  tags=["CHAT BOT API SERVER"]) 
def response_read_data(file_path: str, filename: str, min_chunk_size : int):
    """ë°ì´í„° íŒŒì¼ì„ ì½ê³ , ë²¡í„°í™”í•˜ëŠ” í•¨ìˆ˜"""
    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        # llm = setup_llm_pipeline()  # modelì—ì„œ transformer ì ìš©(GPU memory out)
        llm = ChatOllama(model=model_name) # ê¸°ë³¸ ChatOllama

        # ëª¨ë¸ ì´ë¦„ ë¡œê¹…
        print(f"ì‚¬ìš©ë˜ëŠ” ëª¨ë¸: {llm}")  # ëª¨ë¸ ì´ë¦„ ì¶œë ¥
        
        file_type = filename.split('.')[-1].lower()
        print(file_type)
        
        target_path = f'./templates/{file_path}/{filename}'
        
        print(target_path)
        
        # ìë£Œí˜•ë³„ë¡œ íŒŒì¼ ë¡œë“œ
        if file_type == "pdf":
            loader = PDFPlumberLoader(target_path)
            pages = loader.load()
            
        elif file_type == "xlsx" or file_type == 'xls':
            df = pd.read_excel(target_path)
            
            loader = DataFrameLoader(df, page_content_column='actRmks') # ì—¬ê¸° ì£¼ê¸°ì ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨
            pages = loader.load()
            
        elif file_type == "csv":
            loader = CSVLoader(target_path, encoding='utf-8')
            pages = loader.load()
            
        elif file_type == 'hwp': #hwp5txt
            loader = HWPLoader(target_path)
            pages = loader.load()
            
            # pagesê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´ ê°œë³„ ìš”ì†Œë¥¼ í•©ì³ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
            text = "\n".join(page.page_content for page in pages)
            
            # ì „ì²˜ë¦¬ ì ìš©
            preprocess_text_origin = preprocess_text(text)
            print('=====================')
            print(preprocess_text_origin)
            
        #  format_doc(pages)
        print(pages)
            
        if not pages:
            raise ValueError("íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

        # í…ìŠ¤íŠ¸ ì²­í‚¹
        text_chunks = text_splitter.split_documents(pages)
        
        
        # 3. ìµœì†Œ í¬ê¸° í•„í„°ë§
        filter_text_chunks = [
            doc for doc in text_chunks if len(doc.page_content.strip()) >= int(min_chunk_size)
        ]
        
        
        # Embeddingê³¼ Vector Store ì„¤ì •
        embeddings = OllamaEmbeddings(model=model_name)  # ì‚¬ìš©í•˜ë ¤ëŠ” Embedding ëª¨ë¸
        
        vector_store = Chroma.from_documents(filter_text_chunks, embeddings)
        
        for i, chunk in tqdm(enumerate(filter_text_chunks), total=len(filter_text_chunks), desc="Vectorizing documents"):
            # ê° í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
            vector_store.add_documents([chunk])
        
        print(type(vector_store))
        
        # # Retriever ì„¤ì •(chroma, FAISS, bm25)
        # chroma_retriever = vector_store.as_retriever(
        #     search_type="similarity",
        #     search_kwargs={"k": 3}
        #     )
        
        FAISS_vectorstore = FAISS.from_documents(documents=filter_text_chunks,
                                                embedding=embeddings,
                                                distance_strategy = DistanceStrategy.COSINE,
                                                )
        faiss_retriever = FAISS_vectorstore.as_retriever()
        
        bm25_retriever = BM25Retriever.from_documents(filter_text_chunks)
        bm25_retriever.k = 5
        
        
        # ì•™ìƒë¸” retriever(2ê°œ ì´ìƒ)
        ensemble_retriever = EnsembleRetriever(
            retrievers=[faiss_retriever, bm25_retriever],
            weights=[0.7, 0.3],
        )
        
        # # ë¦¬ë­ì»¤ (ì–´ìš¸ë¦¬ëŠ” ìˆœìœ„ ì¬ì¡°ì • ì‹¤í—˜ ì¤‘)
        # model = CrossEncoder("BAAI/bge-reranker-v2-m3")
        # compressor = CrossEncoderReranker(model=model, top_n=3)
        
        # compression_retriever = ContextualCompressionRetriever(
        #     base_compressor=compressor, base_retriever=ensemble_retriever)
        
        
        # Retriever íƒ€ì… í™•ì¸
        # print(f"Chroma Retriever íƒ€ì…: {type(chroma_retriever)}")
        print(f"FAISS Retriever íƒ€ì…: {type(faiss_retriever)}")
        print(f"BM25Retriver íƒ€ì…: {type(bm25_retriever)}")
        print(f"Ensemble Retriever íƒ€ì…: {type(ensemble_retriever)}")
        # print(f"compression Retriever íƒ€ì…: {type(compression_retriever)}")
        
        # í…œí”Œë¦¿ ì„¤ì •
        prompt = ChatPromptTemplate.from_messages(
            [
                # System Prompt ì ìš© (ìœ„ì—ì„œ ì„¤ê³„í•œ ë²„ì „)
                ("system", """ë‹¹ì‹ ì€ ì¬ë‚œê´€ë¦¬ ì „ë¬¸ê°€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
                PDF ë° TXT ë°ì´í„°ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ í•´ë‹¹ ë°ì´í„°ì˜ ìš”ì•½ ë˜ëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
                ë°ì´í„°ì˜ ë‚´ìš©ì€ 2000ë°”ì´íŠ¸ë¥¼ ë„˜ì§€ ì•Šìœ¼ë©°, ê°„ëµí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                ### ğŸ”¹ **ğŸ“Œ í•µì‹¬ ì›ì¹™**
                1. **ì¬ë‚œì•ˆì „ ê´€ë ¨ ì§ˆë¬¸**: 
                    - ì…ë ¥ë°›ì€ ë°ì´í„° ë‚´ì— ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì´ ì›í•˜ëŠ” ì •ë³´ì˜ ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. 
                    - ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì„¤ëª…(CoT, Chain of Thought ë°©ì‹)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
                    
                2. **íŒŒì¼(PDF, TXT) ì…ë ¥ ì‹œ**:
                    - ì‚¬ìš©ìê°€ ì›í•˜ë©´ **íŒŒì¼ ìš”ì•½, íŠ¹ì • ë‚´ìš© ê²€ìƒ‰ ë° ì •ë¦¬**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    - ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    
                3. **ì–¸ì–´ ì •ì±…**:
                    - ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´**ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.
                    - ì§ˆë¬¸ì— **í•œêµ­ì–´ê°€ í¬í•¨ëœ ê²½ìš°** ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
                    - ì§ˆë¬¸ì´ **í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°**, í•´ë‹¹ ì–¸ì–´ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                
                4. **ë‹µë³€ ìŠ¤íƒ€ì¼**:
                    - **ê³µì†í•˜ê³  ì •ì¤‘í•œ ì–´ì¡°**ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
                """),
                ("human", "ì§ˆë¬¸: {question}")
            ]
        )
        
        # Chain ìƒì„±
        chain = ensemble_retriever | prompt | llm | StrOutputParser() 
        
        response = chain.invoke("ê·¸ë ‡ë‹¤ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ì˜ êµ¬ì„±ê³¼ ì–´ë…¸í…Œì´ì…˜ í¬ë§·ì´ ì–´ë–»ê²Œ ë˜ì–´ìˆëŠ”ì§€ ì•Œë ¤ì¤˜.") 
        
        print(response)
        
        answer = {"answer" : response} #JSON í˜•ì‹ìœ¼ë¡œ ë¦¬í„´
        
        return answer  # ìƒì„±ëœ QA ì²´ì¸ ë°˜í™˜

    except Exception as e:
        print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        raise
     
