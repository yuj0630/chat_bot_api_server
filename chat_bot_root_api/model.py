# transformer ì‚¬ìš©í•˜ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤.
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from langchain_community.llms import huggingface_pipeline
import torch

# ì¶”ê°€ì ì¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ì´ë¯¸ ì¼ë¶€ëŠ” ìœ„ì—ì„œ importë¨)
from transformers import (
    AutoModelForCausalLM, # Googleì˜ Gemma 2B ëª¨ë¸ (ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging
)

# ì‚¬ìš©í•  ëª¨ë¸ ID ì„¤ì • (Beomiì˜ í•œêµ­ì–´-ì˜ì–´ ì§€ì› LLaMA-3 ëª¨ë¸, 8B íŒŒë¼ë¯¸í„° í¬ê¸°)
# model_id = "beomi/llama-3-Open-Ko-8B"
model_id = "BAAI/bge-reranker-v2-m3" # bge-reranker ë²„ì „

# ëª¨ë¸ ì‹¤í–‰ ì‹œ ì‚¬ìš©í•  ì˜µì…˜ ì„¤ì • (í˜„ì¬ CPUì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •ë¨)
model_kwargs = {'device': 'cuda'}

# í† í¬ë‚˜ì´ì € ë¡œë“œ (ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• )
tokenizer = AutoTokenizer.from_pretrained(model_id)  # ì§€ì •í•œ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ

# ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•œ chat_template ì„¤ì •
tokenizer.chat_template = """<|system|>
ë‹¹ì‹ ì€ ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ì „ë¬¸ê°€ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê³µì†í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
PDF ë° TXT ë°ì´í„°ë¥¼ ì…ë ¥ë°›ìœ¼ë©´ í•´ë‹¹ ë°ì´í„°ì˜ ìš”ì•½ ë˜ëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ”¹ **ğŸ“Œ í•µì‹¬ ì›ì¹™**
1. **ì¬ë‚œ ì•ˆì „ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸**: 
    - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ë‚œ ëŒ€ì‘ ë° ì˜ˆë°© ì§€ì¹¨ì„ ì œê³µí•©ë‹ˆë‹¤.
    - ì²´ê³„ì ì¸ ë‹¨ê³„ë³„ ì„¤ëª…(CoT, Chain of Thought ë°©ì‹)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

2. **íŒŒì¼(PDF, TXT) ì…ë ¥ ì‹œ**:
    - ì‚¬ìš©ìê°€ ì›í•˜ë©´ **íŒŒì¼ ìš”ì•½, íŠ¹ì • ë‚´ìš© ê²€ìƒ‰ ë° ì •ë¦¬**ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

3. **ì–¸ì–´ ì •ì±…**:
    - ê¸°ë³¸ì ìœ¼ë¡œ **ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´**ë¡œ ì‘ì„±ë©ë‹ˆë‹¤.
    - ì§ˆë¬¸ì— **í•œêµ­ì–´ê°€ í¬í•¨ëœ ê²½ìš°** ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    - ì§ˆë¬¸ì´ **í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°**, í•´ë‹¹ ì–¸ì–´ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

4. **ë‹µë³€ ìŠ¤íƒ€ì¼**:
    - **ê³µì†í•˜ê³  ì •ì¤‘í•œ ì–´ì¡°**ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
<|end|>

<|user|>{user_input}<|end|>
<|assistant|>"""


def setup_llm_pipeline():
    
    # CPU í™˜ê²½ì—ì„œëŠ” ì–‘ìí™” ì˜µì…˜ ì œê±°
    if torch.cuda.is_available():
        # 4ë¹„íŠ¸ ì–‘ìí™” ì˜µì…˜ 
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            llm_int8_enable_fp32_cpu_offload=False
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto", 
            offload_folder="./offload",  # ëª¨ë¸ì„ ì˜¤í”„ë¡œë“œí•  ë””ìŠ¤í¬ ê²½ë¡œ ì„¤ì •
            trust_remote_code=True
        )
        
        # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
        # model = model.to('cuda')  # ëª…ì‹œì ìœ¼ë¡œ GPUì— ëª¨ë¸ì„ í• ë‹¹
        
    # CPU í™˜ê²½ì—ì„œëŠ” ì–‘ìí™” ì—†ì´ ëª¨ë¸ ë¡œë“œ  
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto", 
            offload_folder="./offload",  # ëª¨ë¸ì„ ì˜¤í”„ë¡œë“œí•  ë””ìŠ¤í¬ ê²½ë¡œ ì„¤ì •
            trust_remote_code=True
        )
    
    # HuggingFacePipeline ê°ì²´ ìƒì„±
    text_generation_pipeline = pipeline(
        model=model,
        tokenizer=tokenizer,
        task="text-generation",
        do_sample=True,
        temperature=0.5,
        top_p=0.5,
        return_full_text=False,
        max_new_tokens=128
    )
    
    llm = text_generation_pipeline
    
    return llm

# # ëª¨ë¸ ì„¤ì • ë³€ê²½ (í›ˆë ¨ ë° ì¶”ë¡  ì‹œ ìºì‹œ ì‚¬ìš© ë¹„í™œì„±í™”)
# model.config.use_cache = False  # ì´ì „ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ì§€ ì•Šë„ë¡ ì„¤ì • (í›ˆë ¨ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ)

# # ëª¨ë¸ì˜ ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹ ì„¤ì • (TPU ë˜ëŠ” ë‹¤ì¤‘ GPUë¥¼ ì‚¬ìš©í•  ê²½ìš° í•„ìš”í•  ìˆ˜ë„ ìˆìŒ)
# model.config.pretraining_tp = 1  # ë³‘ë ¬í™” ë¹„ìœ¨ ì„¤ì • (1ì´ë©´ ê¸°ë³¸ê°’)
